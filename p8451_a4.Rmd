---
title: "P8451 Machine Learning in Public Health - Assignment 1"
output: github_document
date: "2023-2-14"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In preparation for the analyses below, we will load the following libraries:
```{r}
library(caret)
library(tidyverse)
library(dplyr)
```

# Part 0: Data Preprocessing

## Data Import and Cleaning 

We will begin by importing the general health and physical activity data collected by the __New York City Department of Health__ using the `read_csv` function. Next, we will clean the data by first applying the `clean_names` function, then applying the `mutate` function to generate variables with more representative variable names. All variables are initially imported as numeric variables. According to the data codebook provided, the following variables are factor variables with anywhere between 2 to 6 levels: 

* `hypertension` (chronic1)
* `diabetes` (chronic3)
* `asthma` (chornic4)
* `smoking` (tobacco1)
* `alcohol` (alcohol1)
* `physical_activity_minutes` (gpaq8totmin)
* `diet_cat` (habits5)
* `age_cat` (agegroup)
* `sex` (dem3)
* `hispanic` (dem4)
* `born_in_US` (dem8)
* `poverty_group` (povertygroup)

Finally, using the `select` function, we select only the newly labeled variables, remove entries with NA using `na.omit`, and remove any duplicate ID entries using the `distinct` function. 

```{r}
nyc_health = read_csv("./class4_p1.csv") %>% 
  janitor::clean_names() %>% 
  mutate(id = x1, 
         hypertension = factor(chronic1, labels = c("Yes", "No")),
         diabetes = factor(chronic3, labels = c("Yes, No")), 
         asthma = factor(chronic4, labels = c("Yes", "No")), 
         smoking = factor(tobacco1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         alcohol = factor(alcohol1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         physical_activity_minutes = gpaq8totmin, 
         walk_days = gpaq11days, 
         physical_activity_cat = factor(habits5, labels = c("Very Active", 
                                                            "Somewhat Active", 
                                                            "Not Very Active", 
                                                            "Not Active At All")), 
         diet_cat = factor(habits7, labels = c("Excellent",
                                               "Very Good", 
                                               "Good", 
                                               "Fair", 
                                               "Poor")), 
         age_cat = factor(agegroup, labels = c("18-24 Yrs", 
                                               "25-44 Yrs", 
                                               "45-64 Yrs", 
                                               "65+")), 
         sex = factor(dem3, labels = c("Male", "Female")), 
         hispanic = factor(dem4, labels = c("Yes", "No")), 
         born_in_US = factor(dem8, labels = c("USA", "Outside USA")), 
         poverty_group = factor(povertygroup, labels = c("<100%", 
                                                         "100-199%", 
                                                         "200-399%", 
                                                         "400-599%", 
                                                         "600% +", 
                                                         "Don't Know")), 
         healthy_days = healthydays) %>% 
  select(id, hypertension, diabetes, asthma, bmi, smoking, alcohol, 
         physical_activity_minutes, walk_days, physical_activity_cat, diet_cat, 
         age_cat, sex, hispanic, born_in_US, poverty_group, healthy_days) %>% 
  na.omit() %>% 
  distinct(id, .keep_all = TRUE)
```

## Feature Selection: Identifying and Removing Correlated Predictors

Many machine learning algorithms are unable to differentiate between highly correlated features. As such, we want to identify highly correlated features that present the same mathematical information and subsequently remove them, to avoid introducing error in our approach. 

To complete this feature selection process, we will first select only the numeric variables in our `nyc_health` data set, since correlations can only be assessed with numeric variables. We will then apply the `cor` function that will calculate correlations. These calculated correlations will then be fed into the `findCorrelation` function with a cutoff of __0.4__. The features that correlated at 0.4 and above will be stored in a new objected labeled as `high_correlations`.

```{r}
nyc_health_numeric = nyc_health %>% 
  select(where(is.numeric)) 

correlations = cor(nyc_health_numeric, use = "complete.obs")

high_correlations = findCorrelation(correlations, cutoff = 0.4)
```

Since there are no values in the `high_correlations` object, we can conclude that there are no highly correlated variables in these data. 

## Partitioning Data

For the purposes of this analysis, we will partition the data into training and testing using a 70/30 split. This process involves applying the `createDataPartition` function to generate a set of training and testing data with equal proportion of individual with the outcome of interest, i.e., `healthy_days`. The new object `train_index` contains all the indexes of the rows in the original data set contained in the 70% split. The rows indexed to be in the 70% is assigned to a new training data set, and the remaining 30% is assigned to a new testing data set. 

```{r}
train_index = createDataPartition(nyc_health$healthy_days, p = 0.7, list = FALSE)

nyc_health_train <- nyc_health[train_index,]
nyc_health_test <- nyc_health[-train_index,]
```

# Part I: Implementing a Simple Prediction Pipeline

## Question 1: Fitting Prediction Models

In the code chunk below, we construct two linear regression models to predict the number of days in a month an individual reported having good physical health (`healthy_days`). 

In the first model, __`healthy_days_model_1`__, we include features containing basic demographic information (i.e., sex, age category, race, place of birth, and poverty group), as well as medical information such as BMI and history of chronic diseases, specifically for hypertension, diabetes, and asthma. 

In the second model, __`healthy_days_model_2`__, we include the same features containing basic demographic information, as well as self-reported assessments of levels of physical activity and diet.

```{r}
healthy_days_model_1 = 
  nyc_health %>% 
  lm(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + bmi + hypertension + diabetes + asthma, data = .)

healthy_days_model_2 = 
  nyc_health %>% 
  lm(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + bmi + physical_activity_cat + diet_cat, data = .)
```

Below we tabulate the beta estimates, 95% CIs and p-values associated with each feature for the two prediction models generated above.

```{r}
healthy_days_model_1 %>% 
  broom::tidy() %>% 
  mutate(lower_CI = estimate - 1.96*std.error, 
         upper_CI = estimate + 1.96*std.error) %>% 
  select(term, estimate, lower_CI, upper_CI, p.value) %>% 
  knitr::kable(
    col.names = c('Term', 'beta Estimate', 'Lower 95% CI', 'Upper 95% CI', "p-value"),
    digits = 3, 
    caption = "Prediction Model 1 for Healthy Days")

healthy_days_model_2 %>% 
  broom::tidy() %>% 
  mutate(lower_CI = estimate - 1.96*std.error, 
         upper_CI = estimate + 1.96*std.error) %>% 
  select(term, estimate, lower_CI, upper_CI, p.value) %>% 
  knitr::kable(
    col.names = c('Term', 'beta Estimate', 'Lower 95% CI', 'Upper 95% CI', "p-value"),
    digits = 3, 
    caption = "Prediction Model 2 for Healthy Days")
```

## Question 2: Applying Prediction Models Within Test Data 

First, we will use the `trainControl` function to set our validation method. For the purposes of this analysis, we will use the 10-fold crooss validation method.

```{r}
control.settings = 
  trainControl(method = "cv", number = 10)
```

We will now apply these control settings within the `train` function itself, which will be used to implement our algorithms. Below we

```{r}

```


## Question 3: Discussion of Useful Implementation of Final Model 

 

# Part II: Conducting an Unsupervised Analysis
