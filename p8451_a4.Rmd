---
title: "P8451 Machine Learning in Public Health - Assignment 4"
output: github_document
date: "2023-2-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In preparation for all the analyses below, we will load the following libraries:
```{r}
library(caret)
library(tidyverse)
library(dplyr)
library(devtools)
library(stats)
library(factoextra)
library(cluster)
```

# Part 0: Data Preprocessing

## Data Import and Cleaning 

We will begin by importing the general health and physical activity data collected by the __New York City Department of Health__ using the `read_csv` function. Next, we will clean the data by first applying the `clean_names` function, then applying the `mutate` function to generate variables with more representative variable names. All variables are initially imported as numeric variables. According to the data codebook provided, the following variables are factor variables with anywhere between 2 to 6 levels: 

* `hypertension` (chronic1)
* `diabetes` (chronic3)
* `asthma` (chornic4)
* `smoking` (tobacco1)
* `alcohol` (alcohol1)
* `physical_activity_minutes` (gpaq8totmin)
* `diet_cat` (habits5)
* `age_cat` (agegroup)
* `sex` (dem3)
* `hispanic` (dem4)
* `born_in_US` (dem8)
* `poverty_group` (povertygroup)

Finally, using the `select` function, we select only the newly labeled variables, remove entries with NA using `na.omit`, and remove any duplicate ID entries using the `distinct` function. 

```{r}
nyc_health = read_csv("./class4_p1.csv") %>% 
  janitor::clean_names() %>% 
  mutate(id = x1, 
         hypertension = factor(chronic1, labels = c("Yes", "No")),
         diabetes = factor(chronic3, labels = c("Yes, No")), 
         asthma = factor(chronic4, labels = c("Yes", "No")), 
         smoking = factor(tobacco1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         alcohol = factor(alcohol1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         physical_activity_minutes = gpaq8totmin, 
         walk_days = gpaq11days, 
         physical_activity_cat = factor(habits5, labels = c("Very Active", 
                                                            "Somewhat Active", 
                                                            "Not Very Active", 
                                                            "Not Active At All")), 
         diet_cat = factor(habits7, labels = c("Excellent",
                                               "Very Good", 
                                               "Good", 
                                               "Fair", 
                                               "Poor")), 
         age_cat = factor(agegroup, labels = c("18-24 Yrs", 
                                               "25-44 Yrs", 
                                               "45-64 Yrs", 
                                               "65+")), 
         sex = factor(dem3, labels = c("Male", "Female")), 
         hispanic = factor(dem4, labels = c("Yes", "No")), 
         born_in_US = factor(dem8, labels = c("USA", "Outside USA")), 
         poverty_group = factor(povertygroup, labels = c("<100%", 
                                                         "100-199%", 
                                                         "200-399%", 
                                                         "400-599%", 
                                                         "600% +", 
                                                         "Don't Know")), 
         healthy_days = healthydays) %>% 
  select(id, hypertension, diabetes, asthma, bmi, smoking, alcohol, 
         physical_activity_minutes, walk_days, physical_activity_cat, diet_cat, 
         age_cat, sex, hispanic, born_in_US, poverty_group, healthy_days) %>% 
  na.omit() %>% 
  distinct(id, .keep_all = TRUE)
```

## Feature Selection: Identifying and Removing Correlated Predictors

Many machine learning algorithms are unable to differentiate between highly correlated features. As such, we want to identify highly correlated features that present the same mathematical information and subsequently remove them, to avoid introducing error in our approach. 

To complete this feature selection process, we will first select only the numeric variables in our `nyc_health` data set, since correlations can only be assessed with numeric variables. We will then apply the `cor` function that will calculate correlations. These calculated correlations will then be fed into the `findCorrelation` function with a cutoff of __0.4__. The features that correlated at 0.4 and above will be stored in a new objected labeled as `high_correlations`.

```{r}
nyc_health_numeric = nyc_health %>% 
  select(where(is.numeric)) 

correlations = cor(nyc_health_numeric, use = "complete.obs")

high_correlations = findCorrelation(correlations, cutoff = 0.4)
```

Since there are no values in the `high_correlations` object, we can conclude that there are no highly correlated variables in these data. 

## Centering and Scaling

Below, we center and scale these data. In general, it is always good practice to do so! 

```{r}
preprocess_setup <- preProcess(nyc_health_numeric, method = c("center", "scale"))
transformed.vals = predict(preprocess_setup, nyc_health_numeric)
```

## Partitioning Data

For the purposes of this analysis, we will partition the data into training and testing using a 70/30 split. This process involves applying the `createDataPartition` function to generate a set of training and testing data with equal proportion of individual with the outcome of interest, i.e., `healthy_days`. The new object `train_index` contains all the indexes of the rows in the original data set contained in the 70% split. The rows indexed to be in the 70% is assigned to a new training data set, and the remaining 30% is assigned to a new testing data set. 

```{r}
train_index = createDataPartition(nyc_health$healthy_days, p = 0.7, list = FALSE)

nyc_health_train <- nyc_health[train_index,]
nyc_health_test <- nyc_health[-train_index,]
```

# Part I: Implementing a Simple Prediction Pipeline

## Question 1: Fitting Prediction Models

Below, we will fit two prediction models with linear regression to preidct the number of days in a month an individual reported having good physical health (i.e., variable `healthy_days`).

### 1.1 Model 1: Demographics + Medical Conditions 

In the first model, __`healthy_days_model_1`__, we include features containing basic demographic information, as well as those containing medical information. The features included in this model are as follows: 

* `sex` 
* `age_cat` 
* `hispanic`
* `born_in_US`
* `poverty_group`
* `bmi`
* `hypertension` 
* `diabetes` 
* `asthma` 

In the code chunk below, we will use the `trainControl` function to set our validation method. For the purposes of this analysis, we will use the 10-fold cross validation method.

```{r}
control.settings = 
  trainControl(method = "cv", number = 10)
```

These control settings can now be applied within the `train` function, which will be used to implement our algorithms. We also apply a tuning grid for lambda and alpha in order to generate the lambda and alpha values that minimizes the RMSE, and therefore the best model. The parameters of the grid search (i.e., the values of alpha and lambda) were determined by first running a rough grid search, then narrowing down the range of values to yield more precise values. 


```{r}
alpha = seq(0, 1, length = 20)
lambda = seq(0, 0.2, length = 20)
lambda_grid = expand.grid(alpha = alpha, lambda = lambda)

set.seed(123)
healthy_days_model_1 = 
  train(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + bmi + hypertension + diabetes + asthma, data = nyc_health_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

healthy_days_model_1$bestTune
```

### 1.2 Model 2: Demographics + Self-Reported Data

In the second model, __`healthy_days_model_2`__, we include the same features containing basic demographic information, as well as self-reported data regarding smoking and alcohol consumption, levels of physical activity, and diet. The features included in this model are as follows: 

* `sex` 
* `age_cat` 
* `hispanic`
* `born_in_US`
* `poverty_group`
* `smoking`
* `alcohol`
* `physical_activity_minutes`
* `walk_days`
* `physical_activity_cat`
* `diet_cat`

Replicating the approach applied to train the first model above, we will apply the same control settings to the `train` function. We also apply a tuning grid again for lambda and alpha in order to generate the lambda and alpha values that minimizes the RMSE, and therefore the best model. 

```{r}
alpha = seq(0.9, 1, length = 20)
lambda = seq(0, 0.1, length = 20)
lambda_grid = expand.grid(alpha = alpha, lambda = lambda)

set.seed(123)
healthy_days_model_2 = 
  train(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + smoking + alcohol + physical_activity_minutes + walk_days + physical_activity_cat + diet_cat, data = nyc_health_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

healthy_days_model_2$bestTune
```

## Question 2: Applying Prediction Models Within Test Data 

After producing the two trained models above, we can proceed with model evaluation by first applying the prediction models within test data using the `predict` function. We then generate a function that calculates the RMSE, with the model testing outcomes set as the actual/observed values, and the original testing data as the expected values. 

### 2.1 Prediction Model 1 Evaluation

Below we apply the aforementioned steps to evaluate `healthy_days_model_1`. 

```{r}
test_outcome_model_1 = predict(healthy_days_model_1, nyc_health_test)

rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_model_1, nyc_health_test$healthy_days)
```

### 2.2 Prediction Model 2 Evaluation

Below we apply the above mentioned steps to evaluate `healthy_days_model_2`. 

```{r}
test_outcome_model_2 = predict(healthy_days_model_2, nyc_health_test)

rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_model_2, nyc_health_test$healthy_days)
```
Since the RMSE is minimized in `healthy_days_model_2`, we can conclude that __`healthy_days_model_2` is the preferred prediction model__. 

## Question 3: Discussion of Useful Implementation of Final Model 

The features in `healthy_days_model_2` contain mostly self-reported data, which contrasts the some features included in `healthy_days_model_1`, which contains information on medical history. An application of `healthy_days_model_2` may be useful when access to data is limited to qualitative data collected through population surveys. In instances where individual medical history is not able to be ascertained, self-reported data can still be used to predict the number of days in a month an individual reports having good physical health. 

# Part II: Conducting an Unsupervised Analysis

For the purposes of this analysis, we will be using the built-in R data set __USArrests__, which includes the crime statistics for each of the 50 U.S. states in 1973. We can apply the `skim` function to generate general descriptive statistics for the data set. 

```{r}
us_arrests = USArrests
skimr::skim(us_arrests)
```

The `us_arrests` data set contains __`r nrow(us_arrests)` rows__ and __`r ncol(us_arrests)` columns__. Each row corresponds to a U.S. state, and each column contains the following state-specific data:

* Murder arrests per 100,000 population
* Assault arrests per 100,000 population
* Percent urban population
* Rape arrests per 100,000 population 

There are __`r sum(is.na(us_arrests))` missing values__ in these data.

## Question 4: Hierarchical Cluster Analysis 

### 4.1 Assessing Need for Scaling

First, we will obtain and compare the means and standard deviations across all features by applying the `colMeans` function. 

```{r}
colMeans(us_arrests, na.rm = TRUE)
apply(us_arrests, 2, sd, na.rm = TRUE)
```

There are significant differences in the means and standard deviations across the features. It would be most appropriate, therefore, to proceed with centering and scaling these data. To center and scale these data, we will apply the `prcomp` function. 

```{r}
us_arrests = 
  prcomp( ~ ., data = us_arrests, center = TRUE, scale = TRUE, na.action = na.omit)

us_arrests$scale
```

### 4.2 Conducting Hierarchical Clustering Analysis

Below, we first create a dissimilarity matrix by applying the `dist` function. The data frame is inputted into the function, and the Euclidean distance calculation method is entered, as directed. The complete linkage method is applied as well. After creating the dissimilarity matrix, we will then use the `plot` function to generate the obtained dendrogram.

```{r}
dissimilarity_matrix = dist(us_arrests, method = "euclidean")

clusters_h <- hclust(dissimilarity_matrix, method = "complete" )

plot(clusters_h, cex = 0.6, hang = -1)
```
There is now a decision point regarding where in the height we draw the line to determine the optimal number of clusters. To make this decision, we will apply the gap statistic. 

```{r}
hclusCut = function(x, k) list(cluster = cutree(hclust(dist(x, method = "euclidian"), method = "average"), k = k))

gap_stat = clusGap(us_arrests, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```


## Question 5: A Research Application of the Newly Identified Clusters




