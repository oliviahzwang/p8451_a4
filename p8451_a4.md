P8451 Machine Learning in Public Health - Assignment 4
================
2023-2-14

In preparation for all the analyses below, we will load the following
libraries:

``` r
library(caret)
```

    ## Loading required package: ggplot2

    ## Loading required package: lattice

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2
    ## ──

    ## ✔ tibble  3.1.8     ✔ dplyr   1.1.0
    ## ✔ tidyr   1.3.0     ✔ stringr 1.5.0
    ## ✔ readr   2.1.4     ✔ forcats 1.0.0
    ## ✔ purrr   1.0.1     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ✖ purrr::lift()   masks caret::lift()

``` r
library(dplyr)
library(stats)
library(factoextra)
```

    ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa

``` r
library(cluster)
library(ggpubr)
```

# Part 0: Data Preprocessing

## Data Import and Cleaning

We will begin by importing the general health and physical activity data
collected by the **New York City Department of Health** using the
`read_csv` function. Next, we will clean the data by first applying the
`clean_names` function, then applying the `mutate` function to generate
variables with more representative variable names. All variables are
initially imported as numeric variables. According to the data codebook
provided, the following variables are factor variables with anywhere
between 2 to 6 levels:

- `hypertension` (chronic1)
- `diabetes` (chronic3)
- `asthma` (chornic4)
- `smoking` (tobacco1)
- `alcohol` (alcohol1)
- `physical_activity_minutes` (gpaq8totmin)
- `diet_cat` (habits5)
- `age_cat` (agegroup)
- `sex` (dem3)
- `hispanic` (dem4)
- `born_in_US` (dem8)
- `poverty_group` (povertygroup)

Finally, using the `select` function, we select only the newly labeled
variables, remove entries with NA using `na.omit`, and remove any
duplicate ID entries using the `distinct` function.

``` r
nyc_health = read_csv("./class4_p1.csv") %>% 
  janitor::clean_names() %>% 
  mutate(id = x1, 
         hypertension = factor(chronic1, labels = c("Yes", "No")),
         diabetes = factor(chronic3, labels = c("Yes, No")), 
         asthma = factor(chronic4, labels = c("Yes", "No")), 
         smoking = factor(tobacco1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         alcohol = factor(alcohol1, labels = c("Most or All Days", 
                                               "Some Days", 
                                               "Never")), 
         physical_activity_minutes = gpaq8totmin, 
         walk_days = gpaq11days, 
         physical_activity_cat = factor(habits5, labels = c("Very Active", 
                                                            "Somewhat Active", 
                                                            "Not Very Active", 
                                                            "Not Active At All")), 
         diet_cat = factor(habits7, labels = c("Excellent",
                                               "Very Good", 
                                               "Good", 
                                               "Fair", 
                                               "Poor")), 
         age_cat = factor(agegroup, labels = c("18-24 Yrs", 
                                               "25-44 Yrs", 
                                               "45-64 Yrs", 
                                               "65+")), 
         sex = factor(dem3, labels = c("Male", "Female")), 
         hispanic = factor(dem4, labels = c("Yes", "No")), 
         born_in_US = factor(dem8, labels = c("USA", "Outside USA")), 
         poverty_group = factor(povertygroup, labels = c("<100%", 
                                                         "100-199%", 
                                                         "200-399%", 
                                                         "400-599%", 
                                                         "600% +", 
                                                         "Don't Know")), 
         healthy_days = healthydays) %>% 
  select(id, hypertension, diabetes, asthma, bmi, smoking, alcohol, 
         physical_activity_minutes, walk_days, physical_activity_cat, diet_cat, 
         age_cat, sex, hispanic, born_in_US, poverty_group, healthy_days) %>% 
  na.omit() %>% 
  distinct(id, .keep_all = TRUE)
```

    ## New names:
    ## Rows: 3811 Columns: 17
    ## ── Column specification
    ## ──────────────────────────────────────────────────────── Delimiter: "," dbl
    ## (17): ...1, chronic1, chronic3, chronic4, bmi, tobacco1, alcohol1, gpaq8...
    ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ
    ## Specify the column types or set `show_col_types = FALSE` to quiet this message.
    ## • `` -> `...1`

## Feature Selection: Identifying and Removing Correlated Predictors

Many machine learning algorithms are unable to differentiate between
highly correlated features. As such, we want to identify highly
correlated features that present the same mathematical information and
subsequently remove them, to avoid introducing error in our approach.

To complete this feature selection process, we will first select only
the numeric variables in our `nyc_health` data set, since correlations
can only be assessed with numeric variables. We will then apply the
`cor` function that will calculate correlations. These calculated
correlations will then be fed into the `findCorrelation` function with a
cutoff of **0.4**. The features that correlated at 0.4 and above will be
stored in a new objected labeled as `high_correlations`.

``` r
nyc_health_numeric = nyc_health %>% 
  select(where(is.numeric)) 

correlations = cor(nyc_health_numeric, use = "complete.obs")

high_correlations = findCorrelation(correlations, cutoff = 0.4)
```

Since there are no values in the `high_correlations` object, we can
conclude that there are no highly correlated variables in these data.

## Centering and Scaling

Below, we center and scale these data. In general, it is always good
practice to do so!

``` r
preprocess_setup <- preProcess(nyc_health_numeric, method = c("center", "scale"))
transformed.vals = predict(preprocess_setup, nyc_health_numeric)
```

## Partitioning Data

For the purposes of this analysis, we will partition the data into
training and testing using a 70/30 split. This process involves applying
the `createDataPartition` function to generate a set of training and
testing data with equal proportion of individual with the outcome of
interest, i.e., `healthy_days`. The new object `train_index` contains
all the indexes of the rows in the original data set contained in the
70% split. The rows indexed to be in the 70% is assigned to a new
training data set, and the remaining 30% is assigned to a new testing
data set.

``` r
train_index = createDataPartition(nyc_health$healthy_days, p = 0.7, list = FALSE)

nyc_health_train <- nyc_health[train_index,]
nyc_health_test <- nyc_health[-train_index,]
```

# Part I: Implementing a Simple Prediction Pipeline

## Question 1: Fitting Prediction Models

Below, we will fit two prediction models with linear regression to
preidct the number of days in a month an individual reported having good
physical health (i.e., variable `healthy_days`).

### 1.1 Model 1: Demographics + Medical Conditions

In the first model, **`healthy_days_model_1`**, we include features
containing basic demographic information, as well as those containing
medical information. The features included in this model are as follows:

- `sex`
- `age_cat`
- `hispanic`
- `born_in_US`
- `poverty_group`
- `bmi`
- `hypertension`
- `diabetes`
- `asthma`

In the code chunk below, we will use the `trainControl` function to set
our validation method. For the purposes of this analysis, we will use
the 10-fold cross validation method.

``` r
control.settings = 
  trainControl(method = "cv", number = 10)
```

These control settings can now be applied within the `train` function,
which will be used to implement our algorithms. We also apply a tuning
grid for lambda and alpha in order to generate the lambda and alpha
values that minimizes the RMSE, and therefore the best model. The
parameters of the grid search (i.e., the values of alpha and lambda)
were determined by first running a rough grid search, then narrowing
down the range of values to yield more precise values.

``` r
alpha = seq(0, 1, length = 20)
lambda = seq(0, 0.2, length = 20)
lambda_grid = expand.grid(alpha = alpha, lambda = lambda)

set.seed(123)
healthy_days_model_1 = 
  train(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + bmi + hypertension + diabetes + asthma, data = nyc_health_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

healthy_days_model_1$bestTune
```

    ##    alpha    lambda
    ## 14     0 0.1368421

### 1.2 Model 2: Demographics + Self-Reported Data

In the second model, **`healthy_days_model_2`**, we include the same
features containing basic demographic information, as well as
self-reported data regarding smoking and alcohol consumption, levels of
physical activity, and diet. The features included in this model are as
follows:

- `sex`
- `age_cat`
- `hispanic`
- `born_in_US`
- `poverty_group`
- `smoking`
- `alcohol`
- `physical_activity_minutes`
- `walk_days`
- `physical_activity_cat`
- `diet_cat`

Replicating the approach applied to train the first model above, we will
apply the same control settings to the `train` function. We also apply a
tuning grid again for lambda and alpha in order to generate the lambda
and alpha values that minimizes the RMSE, and therefore the best model.

``` r
alpha = seq(0.9, 1, length = 20)
lambda = seq(0, 0.1, length = 20)
lambda_grid = expand.grid(alpha = alpha, lambda = lambda)

set.seed(123)
healthy_days_model_2 = 
  train(healthy_days ~ sex + age_cat + hispanic + born_in_US + poverty_group + smoking + alcohol + physical_activity_minutes + walk_days + physical_activity_cat + diet_cat, data = nyc_health_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

healthy_days_model_2$bestTune
```

    ##     alpha     lambda
    ## 390     1 0.04736842

## Question 2: Applying Prediction Models Within Test Data

After producing the two trained models above, we can proceed with model
evaluation by first applying the prediction models within test data
using the `predict` function. We then generate a function that
calculates the RMSE, with the model testing outcomes set as the
actual/observed values, and the original testing data as the expected
values.

### 2.1 Prediction Model 1 Evaluation

Below we apply the aforementioned steps to evaluate
`healthy_days_model_1`.

``` r
test_outcome_model_1 = predict(healthy_days_model_1, nyc_health_test)

rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_model_1, nyc_health_test$healthy_days)
```

    ## [1] 7.642131

### 2.2 Prediction Model 2 Evaluation

Below we apply the above mentioned steps to evaluate
`healthy_days_model_2`.

``` r
test_outcome_model_2 = predict(healthy_days_model_2, nyc_health_test)

rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_model_2, nyc_health_test$healthy_days)
```

    ## [1] 7.54694

Since the RMSE is minimized in `healthy_days_model_2`, we can conclude
that **`healthy_days_model_2` is the preferred prediction model**.

## Question 3: Discussion of Useful Implementation of Final Model

The features in `healthy_days_model_2` contain mostly self-reported
data, which contrasts the some features included in
`healthy_days_model_1`, which contains information on medical history.
An application of `healthy_days_model_2` may be useful when access to
data is limited to qualitative data collected through population
surveys. In instances where individual medical history is not able to be
ascertained, self-reported data can still be used to predict the number
of days in a month an individual reports having good physical health.

# Part II: Conducting an Unsupervised Analysis

For the purposes of this analysis, we will be using the built-in R data
set **USArrests**, which includes the crime statistics for each of the
50 U.S. states in 1973. We can apply the `skim` function to generate
general descriptive statistics for the data set.

``` r
us_arrests = USArrests
skimr::skim(us_arrests)
```

|                                                  |            |
|:-------------------------------------------------|:-----------|
| Name                                             | us_arrests |
| Number of rows                                   | 50         |
| Number of columns                                | 4          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |            |
| Column type frequency:                           |            |
| numeric                                          | 4          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |            |
| Group variables                                  | None       |

Data summary

**Variable type: numeric**

| skim_variable | n_missing | complete_rate |   mean |    sd |   p0 |    p25 |    p50 |    p75 |  p100 | hist  |
|:--------------|----------:|--------------:|-------:|------:|-----:|-------:|-------:|-------:|------:|:------|
| Murder        |         0 |             1 |   7.79 |  4.36 |  0.8 |   4.08 |   7.25 |  11.25 |  17.4 | ▇▇▅▅▃ |
| Assault       |         0 |             1 | 170.76 | 83.34 | 45.0 | 109.00 | 159.00 | 249.00 | 337.0 | ▆▇▃▅▃ |
| UrbanPop      |         0 |             1 |  65.54 | 14.47 | 32.0 |  54.50 |  66.00 |  77.75 |  91.0 | ▁▆▇▅▆ |
| Rape          |         0 |             1 |  21.23 |  9.37 |  7.3 |  15.08 |  20.10 |  26.17 |  46.0 | ▆▇▅▂▂ |

The `us_arrests` data set contains **50 rows** and **4 columns**. Each
row corresponds to a U.S. state, and each column contains the following
state-specific data:

- Murder arrests per 100,000 population
- Assault arrests per 100,000 population
- Percent urban population
- Rape arrests per 100,000 population

There are **0 missing values** in these data.

## Question 4: Hierarchical Cluster Analysis

### 4.1 Assessing Need for Scaling

First, we will obtain and compare the means and standard deviations
across all features by applying the `colMeans` function.

``` r
colMeans(us_arrests, na.rm = TRUE)
```

    ##   Murder  Assault UrbanPop     Rape 
    ##    7.788  170.760   65.540   21.232

``` r
apply(us_arrests, 2, sd, na.rm = TRUE)
```

    ##    Murder   Assault  UrbanPop      Rape 
    ##  4.355510 83.337661 14.474763  9.366385

``` r
us_arrests
```

    ##                Murder Assault UrbanPop Rape
    ## Alabama          13.2     236       58 21.2
    ## Alaska           10.0     263       48 44.5
    ## Arizona           8.1     294       80 31.0
    ## Arkansas          8.8     190       50 19.5
    ## California        9.0     276       91 40.6
    ## Colorado          7.9     204       78 38.7
    ## Connecticut       3.3     110       77 11.1
    ## Delaware          5.9     238       72 15.8
    ## Florida          15.4     335       80 31.9
    ## Georgia          17.4     211       60 25.8
    ## Hawaii            5.3      46       83 20.2
    ## Idaho             2.6     120       54 14.2
    ## Illinois         10.4     249       83 24.0
    ## Indiana           7.2     113       65 21.0
    ## Iowa              2.2      56       57 11.3
    ## Kansas            6.0     115       66 18.0
    ## Kentucky          9.7     109       52 16.3
    ## Louisiana        15.4     249       66 22.2
    ## Maine             2.1      83       51  7.8
    ## Maryland         11.3     300       67 27.8
    ## Massachusetts     4.4     149       85 16.3
    ## Michigan         12.1     255       74 35.1
    ## Minnesota         2.7      72       66 14.9
    ## Mississippi      16.1     259       44 17.1
    ## Missouri          9.0     178       70 28.2
    ## Montana           6.0     109       53 16.4
    ## Nebraska          4.3     102       62 16.5
    ## Nevada           12.2     252       81 46.0
    ## New Hampshire     2.1      57       56  9.5
    ## New Jersey        7.4     159       89 18.8
    ## New Mexico       11.4     285       70 32.1
    ## New York         11.1     254       86 26.1
    ## North Carolina   13.0     337       45 16.1
    ## North Dakota      0.8      45       44  7.3
    ## Ohio              7.3     120       75 21.4
    ## Oklahoma          6.6     151       68 20.0
    ## Oregon            4.9     159       67 29.3
    ## Pennsylvania      6.3     106       72 14.9
    ## Rhode Island      3.4     174       87  8.3
    ## South Carolina   14.4     279       48 22.5
    ## South Dakota      3.8      86       45 12.8
    ## Tennessee        13.2     188       59 26.9
    ## Texas            12.7     201       80 25.5
    ## Utah              3.2     120       80 22.9
    ## Vermont           2.2      48       32 11.2
    ## Virginia          8.5     156       63 20.7
    ## Washington        4.0     145       73 26.2
    ## West Virginia     5.7      81       39  9.3
    ## Wisconsin         2.6      53       66 10.8
    ## Wyoming           6.8     161       60 15.6

There are significant differences in the means and standard deviations
across the features. It would be most appropriate, therefore, to proceed
with centering and scaling these data. To center and scale these data,
we will apply the `prcomp` function.

``` r
us_arrests_pca = 
  prcomp( ~ ., data = us_arrests, center = TRUE, scale = TRUE, na.action = na.omit)

us_arrests_pca$x
```

    ##                        PC1         PC2         PC3          PC4
    ## Alabama        -0.97566045  1.12200121 -0.43980366  0.154696581
    ## Alaska         -1.93053788  1.06242692  2.01950027 -0.434175454
    ## Arizona        -1.74544285 -0.73845954  0.05423025 -0.826264240
    ## Arkansas        0.13999894  1.10854226  0.11342217 -0.180973554
    ## California     -2.49861285 -1.52742672  0.59254100 -0.338559240
    ## Colorado       -1.49934074 -0.97762966  1.08400162  0.001450164
    ## Connecticut     1.34499236 -1.07798362 -0.63679250 -0.117278736
    ## Delaware       -0.04722981 -0.32208890 -0.71141032 -0.873113315
    ## Florida        -2.98275967  0.03883425 -0.57103206 -0.095317042
    ## Georgia        -1.62280742  1.26608838 -0.33901818  1.065974459
    ## Hawaii          0.90348448 -1.55467609  0.05027151  0.893733198
    ## Idaho           1.62331903  0.20885253  0.25719021 -0.494087852
    ## Illinois       -1.36505197 -0.67498834 -0.67068647 -0.120794916
    ## Indiana         0.50038122 -0.15003926  0.22576277  0.420397595
    ## Iowa            2.23099579 -0.10300828  0.16291036  0.017379470
    ## Kansas          0.78887206 -0.26744941  0.02529648  0.204421034
    ## Kentucky        0.74331256  0.94880748 -0.02808429  0.663817237
    ## Louisiana      -1.54909076  0.86230011 -0.77560598  0.450157791
    ## Maine           2.37274014  0.37260865 -0.06502225 -0.327138529
    ## Maryland       -1.74564663  0.42335704 -0.15566968 -0.553450589
    ## Massachusetts   0.48128007 -1.45967706 -0.60337172 -0.177793902
    ## Michigan       -2.08725025 -0.15383500  0.38100046  0.101343128
    ## Minnesota       1.67566951 -0.62590670  0.15153200  0.066640316
    ## Mississippi    -0.98647919  2.36973712 -0.73336290  0.213342049
    ## Missouri       -0.68978426 -0.26070794  0.37365033  0.223554811
    ## Montana         1.17353751  0.53147851  0.24440796  0.122498555
    ## Nebraska        1.25291625 -0.19200440  0.17380930  0.015733156
    ## Nevada         -2.84550542 -0.76780502  1.15168793  0.311354436
    ## New Hampshire   2.35995585 -0.01790055  0.03648498 -0.032804291
    ## New Jersey     -0.17974128 -1.43493745 -0.75677041  0.240936580
    ## New Mexico     -1.96012351  0.14141308  0.18184598 -0.336121113
    ## New York       -1.66566662 -0.81491072 -0.63661186 -0.013348844
    ## North Carolina -1.11208808  2.20561081 -0.85489245 -0.944789648
    ## North Dakota    2.96215223  0.59309738  0.29824930 -0.251434626
    ## Ohio            0.22369436 -0.73477837 -0.03082616  0.469152817
    ## Oklahoma        0.30864928 -0.28496113 -0.01515592  0.010228476
    ## Oregon         -0.05852787 -0.53596999  0.93038718 -0.235390872
    ## Pennsylvania    0.87948680 -0.56536050 -0.39660218  0.355452378
    ## Rhode Island    0.85509072 -1.47698328 -1.35617705 -0.607402746
    ## South Carolina -1.30744986  1.91397297 -0.29751723 -0.130145378
    ## South Dakota    1.96779669  0.81506822  0.38538073 -0.108470512
    ## Tennessee      -0.98969377  0.85160534  0.18619262  0.646302674
    ## Texas          -1.34151838 -0.40833518 -0.48712332  0.636731051
    ## Utah            0.54503180 -1.45671524  0.29077592 -0.081486749
    ## Vermont         2.77325613  1.38819435  0.83280797 -0.143433697
    ## Virginia        0.09536670  0.19772785  0.01159482  0.209246429
    ## Washington      0.21472339 -0.96037394  0.61859067 -0.218628161
    ## West Virginia   2.08739306  1.41052627  0.10372163  0.130583080
    ## Wisconsin       2.05881199 -0.60512507 -0.13746933  0.182253407
    ## Wyoming         0.62310061  0.31778662 -0.23824049 -0.164976866

### 4.2 Conducting Hierarchical Clustering Analysis

Below, we first create a dissimilarity matrix by applying the `dist`
function. The data frame is inputted into the function, and the
Euclidean distance calculation method is entered, as directed. The
complete linkage method is applied as well. After creating the
dissimilarity matrix, we will then use the `plot` function to generate
the obtained dendrogram.

``` r
dissimilarity_matrix = dist(us_arrests_pca$x, method = "euclidean")

clusters_h <- hclust(dissimilarity_matrix, method = "complete" )

plot(clusters_h, cex = 0.6, hang = -1)
```

![](p8451_a4_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

There is now a decision point regarding where in the height we draw the
line to determine the optimal number of clusters. To make this decision,
we will apply the gap statistic.

``` r
us_arrests_pca
```

    ## Standard deviations (1, .., p=4):
    ## [1] 1.5748783 0.9948694 0.5971291 0.4164494
    ## 
    ## Rotation (n x k) = (4 x 4):
    ##                 PC1        PC2        PC3         PC4
    ## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
    ## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
    ## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
    ## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432

``` r
hclusCut = function(x, k) list(cluster = cutree(hclust(dist(x, method = "euclidian"), method = "average"), k = k))

gap_stat = clusGap(us_arrests_pca$x, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

![](p8451_a4_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

## Question 5: A Research Application of the Newly Identified Clusters
